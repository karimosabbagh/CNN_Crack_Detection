{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/KarimSabbagh25/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "# Import functions from the modules\n",
    "from modules.data_preparation import process_dataset_testing, process_dataset_training, authenticate_kaggle,  download_dataset\n",
    "from modules.model import build_model, load_model\n",
    "from modules.training import train_model, train_model_k_fold\n",
    "from modules.visualization import visualize_results\n",
    "from modules.test import evaluate_model_on_test_set, evaluate_model_on_test_set_w_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"oluwaseunad/concrete-and-pavement-crack-images\"\n",
    "dataset_zip_path = \"data/D2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "authenticate_kaggle()\n",
    "download_dataset(dataset_name, dataset_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset\n",
    "dataloaders, dataset_sizes, class_names, device = process_dataset_training(dataset_zip_path, subset_fraction=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model based on trained parameters from D1\n",
    "model_path = 'CNN_models/D1_params.pt'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, criterion, optimizer, exp_lr_scheduler = build_model()\n",
    "model = load_model(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "--------------------\n",
      "Epoch 0/5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train the model\n",
    "train_loader, val_loader, test_loader = dataloaders.values()\n",
    "\n",
    "#############################################################\n",
    "# Train with k folds\n",
    "#############################################################\n",
    "model, results = train_model_k_fold(model,\n",
    "                   dataloaders,\n",
    "                   criterion,\n",
    "                   k = 5,\n",
    "                   num_epochs = 6,\n",
    "                   batch_size = 32,\n",
    "                   device='cpu')\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# Train with no folds\n",
    "#############################################################\n",
    "# model, best_acc, results = train_model(\n",
    "#             model = model, \n",
    "#             criterion = criterion,\n",
    "#             optimizer = optimizer, \n",
    "#             scheduler = exp_lr_scheduler,\n",
    "#             dataloaders = dataloaders,\n",
    "#             dataset_sizes = dataset_sizes,\n",
    "#             device =  device,\n",
    "#             num_epochs = 3,\n",
    "#             params_file = \"params.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, all_labels, all_preds = evaluate_model_on_test_set_w_confusion(model, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_full_background_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Plot an enhanced confusion matrix with actual counts and percentages,\n",
    "    highlighting TP/TN cells in full green and FP/FN cells in full red.\n",
    "\n",
    "    Args:\n",
    "        y_true (list or np.array): True labels.\n",
    "        y_pred (list or np.array): Predicted labels.\n",
    "        class_names (list): List of class names (e.g., ['Positive', 'Negative']).\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    total_samples = np.sum(cm)  # Total samples in the dataset\n",
    "\n",
    "    # Calculate percentages relative to the entire dataset\n",
    "    cm_percentage = cm / total_samples * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Colors: Green for TP/TN, Red for FP/FN\n",
    "    colors = np.array([\n",
    "        [\"green\" if i == j else \"red\" for j in range(cm.shape[1])]\n",
    "        for i in range(cm.shape[0])\n",
    "    ])\n",
    "\n",
    "    # Plot the confusion matrix with full background colors\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.add_patch(plt.Rectangle((j - 0.5, i - 0.5), 1, 1, color=colors[i, j], alpha=0.7))\n",
    "\n",
    "            # Display counts and percentages\n",
    "            count_text = f\"{cm[i, j]}\"  # Actual count\n",
    "            percent_text = f\"\\n({cm_percentage[i, j]:.1f}%)\"  # Percentage\n",
    "            text_color = \"white\" if colors[i, j] in [\"green\", \"red\"] else \"black\"\n",
    "            ax.text(j, i, count_text + percent_text, ha=\"center\", va=\"center\", color=text_color, fontsize=12)\n",
    "\n",
    "    # Set labels and ticks\n",
    "    ax.set(\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        xticklabels=[f'Predicted: {cls}' for cls in class_names],\n",
    "        yticklabels=[f'Actual: {cls}' for cls in class_names],\n",
    "        title=\"Confusion Matrix with Full Color Background\",\n",
    "        ylabel=\"Actual Value\",\n",
    "        xlabel=\"Predicted Value\",\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Draw grid and finalize layout\n",
    "    ax.set_xlim(-0.5, cm.shape[1] - 0.5)\n",
    "    ax.set_ylim(-0.5, cm.shape[0] - 0.5)\n",
    "    ax.grid(False)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "y_true = [0, 0, 1, 1, 0, 1, 0, 0, 1, 1] * 50  # Replace with your actual labels\n",
    "y_pred = [0, 1, 1, 1, 0, 0, 0, 0, 1, 1] * 50  # Replace with your actual predictions\n",
    "class_names = [\"No\", \"Yes\"]  # Replace with your class names\n",
    "\n",
    "plot_full_background_confusion_matrix(y_true, y_pred, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_full_background_confusion_matrix_conventional(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Plot an enhanced confusion matrix with actual counts and percentages,\n",
    "    following the conventional layout where:\n",
    "        - TP: Top-left\n",
    "        - TN: Bottom-right\n",
    "        - FP: Top-right\n",
    "        - FN: Bottom-left\n",
    "\n",
    "    Args:\n",
    "        y_true (list or np.array): True labels.\n",
    "        y_pred (list or np.array): Predicted labels.\n",
    "        class_names (list): List of class names (e.g., ['Positive', 'Negative']).\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    total_samples = np.sum(cm)  # Total samples in the dataset\n",
    "\n",
    "    # Calculate percentages relative to the entire dataset\n",
    "    cm_percentage = cm / total_samples * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Colors: Green for TP/TN, Red for FP/FN\n",
    "    colors = np.array([\n",
    "        [\"green\", \"red\"],  # Top-left: TP, Top-right: FP\n",
    "        [\"red\", \"green\"]   # Bottom-left: FN, Bottom-right: TN\n",
    "    ])\n",
    "\n",
    "    # Plot the confusion matrix with full background colors\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.add_patch(plt.Rectangle((j - 0.5, i - 0.5), 1, 1, color=colors[i, j], alpha=0.7))\n",
    "\n",
    "            # Display counts and percentages\n",
    "            count_text = f\"{cm[i, j]}\"  # Actual count\n",
    "            percent_text = f\"\\n({cm_percentage[i, j]:.1f}%)\"  # Percentage\n",
    "            text_color = \"white\"\n",
    "            ax.text(j, i, count_text + percent_text, ha=\"center\", va=\"center\", color=text_color, fontsize=12)\n",
    "\n",
    "    # Set labels and ticks\n",
    "    ax.set(\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        xticklabels=[f'Predicted: {cls}' for cls in class_names],\n",
    "        yticklabels=[f'Actual: {cls}' for cls in class_names],\n",
    "        title=\"Confusion Matrix (Conventional Layout)\",\n",
    "        ylabel=\"Actual Value\",\n",
    "        xlabel=\"Predicted Value\",\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Draw grid and finalize layout\n",
    "    ax.set_xlim(-0.5, cm.shape[1] - 0.5)\n",
    "    ax.set_ylim(-0.5, cm.shape[0] - 0.5)\n",
    "    ax.grid(False)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "y_true = [0, 0, 1, 1, 0, 1, 0, 0, 1, 1] * 50  # Replace with your actual labels\n",
    "y_pred = [0, 1, 1, 1, 0, 0, 0, 0, 1, 1] * 50  # Replace with your actual predictions\n",
    "class_names = [\"Positive\", \"Negative\"]  # Replace with your class names\n",
    "\n",
    "plot_full_background_confusion_matrix_conventional(y_true, y_pred, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_full_background_confusion_matrix_conventional(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Plot an enhanced confusion matrix with actual counts and percentages,\n",
    "    following the conventional layout:\n",
    "        - TP: Top-left\n",
    "        - FP: Top-right\n",
    "        - FN: Bottom-left\n",
    "        - TN: Bottom-right\n",
    "\n",
    "    Args:\n",
    "        y_true (list or np.array): True labels.\n",
    "        y_pred (list or np.array): Predicted labels.\n",
    "        class_names (list): List of class names (e.g., ['Positive', 'Negative']).\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Rearrange confusion matrix to match conventional layout\n",
    "    # [[TP, FP],\n",
    "    #  [FN, TN]]\n",
    "    cm = np.array([[cm[1, 1], cm[0, 1]],\n",
    "                   [cm[1, 0], cm[0, 0]]])\n",
    "    \n",
    "    total_samples = np.sum(cm)  # Total samples in the dataset\n",
    "\n",
    "    # Calculate percentages relative to the entire dataset\n",
    "    cm_percentage = cm / total_samples * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Colors: Green for TP/TN, Red for FP/FN\n",
    "    colors = np.array([\n",
    "        [\"green\", \"red\"],  # Top-left: TP, Top-right: FP\n",
    "        [\"red\", \"green\"]   # Bottom-left: FN, Bottom-right: TN\n",
    "    ])\n",
    "\n",
    "    # Plot the confusion matrix with full background colors\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.add_patch(plt.Rectangle((j - 0.5, i - 0.5), 1, 1, color=colors[i, j], alpha=0.7))\n",
    "\n",
    "            # Display counts and percentages\n",
    "            count_text = f\"{cm[i, j]}\"  # Actual count\n",
    "            percent_text = f\"\\n({cm_percentage[i, j]:.1f}%)\"  # Percentage\n",
    "            text_color = \"white\"\n",
    "            ax.text(j, i, count_text + percent_text, ha=\"center\", va=\"center\", color=text_color, fontsize=12)\n",
    "\n",
    "    # Set labels and ticks\n",
    "    ax.set(\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        xticklabels=[\"Positive\", \"Negative\"],  # Predicted\n",
    "        yticklabels=[\"Positive\", \"Negative\"],  # Actual\n",
    "        title=\"Confusion Matrix (Conventional Layout)\",\n",
    "        ylabel=\"Actual Value\",\n",
    "        xlabel=\"Predicted Value\",\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Draw grid and finalize layout\n",
    "    ax.set_xlim(-0.5, cm.shape[1] - 0.5)\n",
    "    ax.set_ylim(-0.5, cm.shape[0] - 0.5)\n",
    "    ax.grid(False)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "y_true = [0, 0, 1, 1, 0, 1, 0, 0, 1, 1] * 50  # Replace with your actual labels\n",
    "y_pred = [0, 1, 1, 1, 0, 0, 0, 0, 1, 1] * 50  # Replace with your actual predictions\n",
    "class_names = [\"Positive\", \"Negative\"]  # Replace with your class names\n",
    "\n",
    "plot_full_background_confusion_matrix_conventional(y_true, y_pred, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
